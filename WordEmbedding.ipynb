{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word embedding task\n",
    "\n",
    "Word embedding이란 단어의 단어의 뜻을 벡터로 표현하는 방법입니다. \n",
    "<img src=\"files/wv0.PNG\">\n",
    "\n",
    "단어 embedding 방법은 여러 가지가 있지만 본 task에서는 가장 기본적인 Word2Vec을 사용하여 학습을 진행하겠습니다. Word2Vec은 아래의 그림과 같이 주변 단어의 context를 고려하여 현재 위치의 단어를 예측하는 방식으로 학습합니다.\n",
    "\n",
    "<img src=\"files/wv1.PNG\">\n",
    "\n",
    "Word2Vec 모델 구현에는 `gensim` 라이브러리를 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load corpus\n",
    "학습 코퍼스는 위키피디아 문서를 형태소 분석을 하여 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "287875\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['지미', '카터'],\n",
       " ['제임스',\n",
       "  '얼',\n",
       "  '\"',\n",
       "  '지미',\n",
       "  '\"',\n",
       "  '카터',\n",
       "  '주니어',\n",
       "  '(,',\n",
       "  '1924년',\n",
       "  '10월',\n",
       "  '1일',\n",
       "  '~',\n",
       "  ')',\n",
       "  '는',\n",
       "  '민주당',\n",
       "  '출신',\n",
       "  '미국',\n",
       "  '39',\n",
       "  '번째',\n",
       "  '대통령',\n",
       "  '(',\n",
       "  '1977년',\n",
       "  '~',\n",
       "  '1981년',\n",
       "  ')',\n",
       "  '이다',\n",
       "  '.'],\n",
       " ['지미',\n",
       "  '카터',\n",
       "  '는',\n",
       "  '조지아주',\n",
       "  '섬터',\n",
       "  '카운티',\n",
       "  '플레인스',\n",
       "  '마을',\n",
       "  '에서',\n",
       "  '태어났다',\n",
       "  '.',\n",
       "  '조지아',\n",
       "  '공과',\n",
       "  '대학교',\n",
       "  '를',\n",
       "  '졸업',\n",
       "  '하였다',\n",
       "  '.',\n",
       "  '그',\n",
       "  '후',\n",
       "  '해군',\n",
       "  '에',\n",
       "  '들어가',\n",
       "  '전함',\n",
       "  '·',\n",
       "  '원자력',\n",
       "  '·',\n",
       "  '잠수함',\n",
       "  '의',\n",
       "  '승무원',\n",
       "  '으로',\n",
       "  '일',\n",
       "  '하였다',\n",
       "  '.',\n",
       "  '1953년',\n",
       "  '미국',\n",
       "  '해군',\n",
       "  '대위',\n",
       "  '로',\n",
       "  '예',\n",
       "  '편하였고',\n",
       "  '이후',\n",
       "  '땅콩',\n",
       "  '·',\n",
       "  '면화',\n",
       "  '등',\n",
       "  '을',\n",
       "  '가꿔',\n",
       "  '많은',\n",
       "  '돈',\n",
       "  '을',\n",
       "  '벌었다',\n",
       "  '.',\n",
       "  '그',\n",
       "  '의',\n",
       "  '별명',\n",
       "  '이',\n",
       "  '\"',\n",
       "  '땅콩',\n",
       "  '농부',\n",
       "  '\"',\n",
       "  '(',\n",
       "  'Peanut',\n",
       "  'Farmer',\n",
       "  ')',\n",
       "  '로',\n",
       "  '알려졌다',\n",
       "  '.']]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_file_name = \"w2v/wiki_okt.txt\"\n",
    "corpus = []\n",
    "with open(corpus_file_name, encoding=\"UTF8\") as f:\n",
    "    lines = f.readlines()\n",
    "    print(len(lines))\n",
    "    for line in lines:\n",
    "        corpus.append(line.strip().split())\n",
    "\n",
    "corpus[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "모델을 학습시키는 것은 간단합니다. 위에서 준비한 문서 list를 Word2Vec 모델에 바로 입력하면 됩니다.\n",
    "\n",
    "그 외 hyperparameter를 간단히 설명하자면\n",
    "* size: 단어 embedding 크기. 100으로 설정하면 한 단어에 100 dimension을 가지는 벡터를 얻을 수 있습니다.\n",
    "* window: 단어 학습 시 고려할 주변 context 단어 개수\n",
    "* min_count: 학습 corpus에 최소 min_count만큼 등장해야 최종 vocabulary list에 등록합니다.\n",
    "* workers: 학습 processor 개수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(corpus, size=100, window=5, min_count=1, workers=4)\n",
    "model.save(\"w2v.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing\n",
    "#### Representation\n",
    "모델 학습이 끝나면 어떻게 단어가 표현되는지를 알아봅니다.\n",
    "\n",
    "먼저, 단어 벡터를 불러오는 방식은 `model.wv[단어]`로 불러올 수 있습니다. 예를 들어, \"하이닉스\"라는 단어의 벡터를 알고 싶다면 아래와 같이 코드를 짜면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.05458402,  0.16462323, -0.08330727,  0.01520697, -0.01744578,\n",
       "        0.06041553,  0.09409352, -0.0339956 , -0.03857254,  0.10240825,\n",
       "       -0.01074888, -0.07581771, -0.096007  , -0.0561431 ,  0.06756113,\n",
       "        0.06963943, -0.12206154,  0.01840958,  0.0064967 , -0.03991486,\n",
       "       -0.18301414, -0.05180284, -0.00646087, -0.13990559, -0.07753476,\n",
       "       -0.06313062, -0.03087379,  0.06503866,  0.02186177,  0.2039582 ,\n",
       "       -0.12392884, -0.10446624, -0.13538587,  0.01438549, -0.0498361 ,\n",
       "        0.01280456,  0.08030511,  0.21595997,  0.05607784, -0.02687987,\n",
       "        0.1178019 ,  0.01741406,  0.04175974,  0.09708944,  0.07883959,\n",
       "        0.03563969,  0.00568025,  0.12903568,  0.07814391, -0.04740427,\n",
       "        0.0579128 , -0.01110623, -0.0206186 , -0.01419968, -0.16259645,\n",
       "       -0.00653264,  0.04595016,  0.0495457 , -0.00100294,  0.09331138,\n",
       "        0.1140324 , -0.02521081, -0.01926421, -0.07037614, -0.11623224,\n",
       "        0.00197264, -0.10430788,  0.18616222, -0.09523881, -0.08277446,\n",
       "        0.0737093 , -0.08587237,  0.00902717,  0.05697481,  0.12090602,\n",
       "        0.08187362, -0.02988076, -0.03105801,  0.13300794, -0.03698305,\n",
       "        0.10575792,  0.04750875,  0.07024797,  0.11299799,  0.06599905,\n",
       "       -0.12776959,  0.02980402,  0.00892097,  0.14239714, -0.15603997,\n",
       "       -0.05760872, -0.10309568,  0.02006594,  0.07833689, -0.01684047,\n",
       "        0.12680319, -0.01131532,  0.01927482,  0.03847706, -0.03172174],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv[\"하이닉스\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 결과가 \"하이닉스\" 에 대한 벡터입니다. 하지만 이것만 보고는 단어의 뜻을 알기도 어렵고, 컴퓨터가 단어의 뜻을 제대로 학습했는지 알 수 없습니다. 따라서, 이 과정이 실제로 의미가 있는지 아래의 task를 통해 알아보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Similarity\n",
    "Gensim의 word2vec 라이브러리는 다양한 similarity function을 지원합니다. 먼저, \"하이닉스\" 와 비슷한 단어를 찾아보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('텔레콤', 0.7518603801727295),\n",
       " ('브로드밴드', 0.7417868971824646),\n",
       " ('칼텍스', 0.7385914921760559),\n",
       " ('유플러스', 0.737071692943573),\n",
       " ('GS', 0.7230042815208435),\n",
       " ('SDS', 0.7191774845123291),\n",
       " ('케미칼', 0.6841263771057129),\n",
       " ('닛산', 0.6726300120353699),\n",
       " ('아반떼', 0.6720921993255615),\n",
       " ('BMW', 0.6636067628860474)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similar_by_word(\"하이닉스\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SK의 다른 회사들이 상위권에 있는 것을 확인할 수 있습니다.\n",
    "\n",
    "다음으로, 단어의 벡터를 더하고 빼는 방식으로 단어의 유사도를 측정할 수 있습니다. 예를 들어, 야구팀 \"롯데 자이언츠\" 에서 \"롯데\" 를 빼고 \"SK\" 를 넣으면 \"와이번스\" 가 나오면 좋을 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('와이번스', 0.8660757541656494),\n",
       " ('타이거즈', 0.8638233542442322),\n",
       " ('KIA', 0.8458287715911865),\n",
       " ('해태', 0.8379089832305908),\n",
       " ('히어로즈', 0.8166918754577637),\n",
       " ('트윈스', 0.8111632466316223),\n",
       " ('넥센', 0.8111244440078735),\n",
       " ('이글스', 0.801300585269928),\n",
       " ('LG', 0.7984895706176758),\n",
       " ('베어스', 0.7875518798828125)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=[\"자이언츠\", \"SK\"], negative=[\"롯데\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "마지막으로, SK와 하이닉스가 얼마나 유사한지 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6111681"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity(\"SK\", \"하이닉스\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 학습된 단어 벡터를 가지고 다른 task를 수행해 보겠습니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
